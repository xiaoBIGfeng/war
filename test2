import os
import os.path as osp
import glob
import logging
import numpy as np
import cv2
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

import utils.util as util
import data.util as data_util
import models.modules.sttrans as sttrans


def setup_ddp(rank, world_size, port="12345"):
    """
    初始化分布式环境
    """
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = port
    dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)


def cleanup_ddp():
    """销毁分布式环境"""
    dist.destroy_process_group()


def split_data(data_list, world_size, rank):
    """
    根据 rank 将数据集划分到各 GPU
    """
    total_len = len(data_list)
    per_rank_len = (total_len + world_size - 1) // world_size
    start_idx = rank * per_rank_len
    end_idx = min(start_idx + per_rank_len, total_len)
    return data_list[start_idx:end_idx]


def main_worker(rank, world_size, batch_size):
    """
    每个进程的主函数
    """
    setup_ddp(rank, world_size)

    scale = 4
    N_ot = 7  # Temporal frames
    N_in = 1 + N_ot // 2

    model_path = '/media/DATA2/wzr/ECCV_comparisons/github_upload/STDAN/experiments/pretrained_model/STDAN_20220306.pth'
    model = sttrans.STTrans2(scale=4, n_inputs=4, n_outputs=7, nf=64, embed_dim=72, window_size=8)

    data_mode = 'Vimeo'
    test_dataset_folder = '/media/DATA2/wzr/vimeo_septuplet'
    flip_test = False  # Optional
    crop_border = 0

    save_imgs = True
    padding = 'zero padding'

    # 日志设置
    if rank == 0:
        save_folder = f'../results/{data_mode}'
        util.mkdirs(save_folder)
        util.setup_logger('base', save_folder, 'test', level=logging.INFO, screen=True, tofile=True)
        logger = logging.getLogger('base')
        logger.info('Data: {} - {}'.format(data_mode, test_dataset_folder))
        logger.info('Padding mode: {}'.format(padding))
        logger.info('Save images: {}'.format(save_imgs))
        logger.info('Flip Test: {}'.format(flip_test))
        logger.info('Using batch size: {}'.format(batch_size))
    else:
        logger = None

    # 将模型加载到当前 GPU 上
    device = torch.device(f'cuda:{rank}')
    model = model.to(device)

    # 包装模型为分布式并行
    model = DDP(model, device_ids=[rank])

    def single_forward(model, imgs_in):
        """
        处理一个 batch 的输入
        """
        with torch.no_grad():
            b, n, c, h, w = imgs_in.size()
            h_n = int(8 * np.ceil(h / 8))
            w_n = int(8 * np.ceil(w / 8))
            imgs_temp = imgs_in.new_zeros(b, n, c, h_n, w_n)
            imgs_temp[:, :, :, 0:h, 0:w] = imgs_in
            model_output = model(imgs_temp)
            model_output = model_output[:, :, :, 0:scale * h, 0:scale * w]
            if isinstance(model_output, list) or isinstance(model_output, tuple):
                output = model_output[0]
            else:
                output = model_output
        return output

    # 读取测试数据子集
    test_dataset_folder_LR = os.path.join(test_dataset_folder, 'LR_test')
    with open(test_dataset_folder + '/slow_testset.txt', 'r') as f:
        sub_folder_l_list = f.read().splitlines()

    sub_folder_l = []
    for sub_folder in sub_folder_l_list:
        sub_folder_l.append(os.path.join(test_dataset_folder_LR, sub_folder))

    # 按 rank 划分数据
    sub_folder_l = split_data(sub_folder_l, world_size, rank)

    model.load_state_dict(torch.load(model_path, map_location=device), strict=True)
    model.eval()

    avg_psnr_l = []
    avg_ssim_l = []

    for sub_folder in sub_folder_l:
        gt_tested_list = []
        img_LR_l = sorted(glob.glob(sub_folder + '/*'))

        save_sub_folder = osp.join(f'../results/{data_mode}', osp.basename(sub_folder))
        if save_imgs and rank == 0:
            util.mkdirs(save_sub_folder)

        imgs = util.read_seq_imgs(sub_folder)
        img_GT_l = []
        sub_folder_GT = osp.join(sub_folder.replace('/LR_test/', '/sequences/test/'))
        for img_GT_path in sorted(glob.glob(osp.join(sub_folder_GT, '*'))):
            img_GT_l.append(util.read_image(img_GT_path))

        avg_psnr_sum, avg_ssim_sum, cal_n = 0, 0, 0

        # 修改：支持批量处理
        select_idx_list = util.test_index_generation(True, N_ot, len(img_LR_l))
        for i in range(0, len(select_idx_list), batch_size):  # 每次取 batch_size 个索引
            batch_idxs = select_idx_list[i:i + batch_size]
            imgs_in_batch = []
            gt_idx_batch = []

            for select_idxs in batch_idxs:
                select_idx = select_idxs[0]
                gt_idx = select_idxs[1]
                imgs_in = imgs.index_select(0, torch.LongTensor(select_idx))
                imgs_in_batch.append(imgs_in)
                gt_idx_batch.extend(gt_idx)

            # 拼接 batch
            imgs_in_batch = torch.stack(imgs_in_batch).to(device)

            # 模型推理
            output_batch = single_forward(model, imgs_in_batch)

            # 后处理和指标计算
            outputs = output_batch.data.float().cpu()
            for idx, name_idx in enumerate(gt_idx_batch):
                output_f = outputs[idx, :, :, :].squeeze(0)
                output = util.tensor2img(output_f)

                GT = np.copy(img_GT_l[name_idx])
                if crop_border > 0:
                    cropped_output = output[crop_border:-crop_border, crop_border:-crop_border, :]
                    cropped_GT = GT[crop_border:-crop_border, crop_border:-crop_border, :]
                else:
                    cropped_output = output
                    cropped_GT = GT

                crt_psnr = util.calculate_psnr(cropped_output * 255, cropped_GT * 255)
                avg_psnr_sum += crt_psnr

                crt_ssim = util.calculate_ssim(cropped_output, cropped_GT)
                avg_ssim_sum += crt_ssim

                cal_n += 1

        avg_psnr = avg_psnr_sum / cal_n
        avg_ssim = avg_ssim_sum / cal_n

        avg_psnr_l.append(avg_psnr)
        avg_ssim_l.append(avg_ssim)

    # 汇总数据（主进程打印最终结果）
    if rank == 0:
        total_avg_psnr = sum(avg_psnr_l) / len(avg_psnr_l)
        total_avg_ssim = sum(avg_ssim_l) / len(avg_ssim_l)
        logger.info('Total Average PSNR: {:.6f} dB'.format(total_avg_psnr))
        logger.info('Total Average SSIM: {:.6f}'.format(total_avg_ssim))

    cleanup_ddp()


if __name__ == '__main__':
    world_size = torch.cuda.device_count()  # GPU 数量
    batch_size = 4  # 可配置的 batch size
    torch.multiprocessing.spawn(main_worker, args=(world_size, batch_size), nprocs=world_size)
